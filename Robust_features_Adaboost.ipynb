{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!! Run Combine_features.ipynb before running this file !!!. \n",
    "\n",
    "### Then execute cell by cell for part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nonfaces=np.load('features_nonfaces.npy')\n",
    "features_faces=np.load('features_faces.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_stump(feature_val,y,m,weights,false_neg_penalty=0.5,false_pos_penalty=0.5,err_atol=1e-08,err_rtol=1e-02):\n",
    "    feature_val_inds = feature_val.argsort()\n",
    "    feature_val = feature_val[feature_val_inds[::1]]\n",
    "    y_sorted = y[feature_val_inds[::1]]\n",
    "    my_array=np.unique(feature_val)\n",
    "    Tau=(my_array+np.roll(my_array,shift=-1))/2\n",
    "    Tau=np.concatenate(([my_array[0]-1],Tau))\n",
    "    Tau[-1]=my_array[-1]+1\n",
    "    Margin = np.concatenate(([1],np.roll(my_array,shift=-1)-my_array))\n",
    "    Margin[-1]=1\n",
    "    #weights=weights*(false_neg_penalty+false_pos_penalty)*2\n",
    "    error_samples=np.bitwise_xor(np.greater_equal(feature_val,Tau[:,np.newaxis])\\\n",
    "                           ,(y_sorted>0)[np.newaxis,:])\n",
    "    weighted_err=np.multiply.reduce([error_samples, weights[np.newaxis,:],\\\n",
    "                                     np.where(y_sorted==1,false_neg_penalty,false_pos_penalty)[np.newaxis,:]],0).sum(axis=1)\n",
    "    Toggle=np.greater(weighted_err,0.5)\n",
    "\n",
    "    if np.any(Toggle):\n",
    "        error_samples=np.bitwise_xor(error_samples,Toggle[:,np.newaxis]).astype('bool')\n",
    "        weighted_err=np.multiply.reduce([error_samples, weights[np.newaxis,:],\\\n",
    "                                     np.where(y_sorted==1,false_neg_penalty,false_pos_penalty)[np.newaxis,:]],0).sum(axis=1)\n",
    "\n",
    "\n",
    "    #min_err_index = np.where(np.isclose(weighted_err,weighted_err[np.argmax(np.absolute(weighted_err-0.5))]\\\n",
    "    #                                ,rtol=err_rtol, atol=err_atol))\n",
    "    min_err_index = np.where(np.equal(weighted_err,weighted_err[np.argmax(np.absolute(weighted_err-0.5))]))\n",
    "    \n",
    "    min_err_index=np.argmax(Margin==np.max(Margin[min_err_index]))\n",
    "    min_err_Tau=Tau[min_err_index]\n",
    "    min_err_Margin=Margin[min_err_index]\n",
    "    false_neg=np.multiply(error_samples,(y_sorted==1)[np.newaxis,:]).sum(axis=1) #false neg\n",
    "    false_pos=np.multiply(error_samples,(y_sorted!=1)[np.newaxis,:]).sum(axis=1) #false pos\n",
    "    return_array = np.array([false_neg[min_err_index],false_pos[min_err_index],\\\n",
    "                     min_err_Margin,min_err_Tau,min_err_index,weighted_err[min_err_index],Toggle[min_err_index]])\n",
    "    del Margin\n",
    "    del Tau\n",
    "    del error_samples\n",
    "    del weighted_err\n",
    "    del Toggle\n",
    "    del feature_val_inds\n",
    "    del feature_val\n",
    "    del my_array\n",
    "    del y_sorted\n",
    "    return return_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_union=np.load('array_union.npy').astype('int32')\n",
    "array_union_sel=np.empty((10,2)).astype('int32')\n",
    "for k in range(1,10) :\n",
    "        array_union_sel[k-1,:] = np.array([(k-1)*1000,k*1000]).astype('int32')\n",
    "array_union_sel[-1,:]=np.array([k*1000,9358]).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_examples=1000\n",
    "train_sel = np.reshape([1500+np.arange(num_examples/2),-np.arange(1,num_examples/2+1)[::-1]],-1).astype('int32')\n",
    "\n",
    "y = np.concatenate((-1*np.ones(features_nonfaces.shape[0]),np.ones(features_faces.shape[0])))\n",
    "X = np.vstack([features_nonfaces,features_faces])\n",
    "\n",
    "X= X[train_sel,:]\n",
    "y=y[train_sel]\n",
    "\n",
    "#del features_nonfaces\n",
    "#del features_faces\n",
    "\n",
    "d=X.shape[1]\n",
    "m=X.shape[0]\n",
    "\n",
    "weights_init=(np.ones((m))/m)\n",
    "feature_sel=np.empty((64,2)).astype('int32')\n",
    "\n",
    "for k in range(1,64) :\n",
    "        feature_sel[k-1,:] = np.array([(k-1)*1000,k*1000]).astype('int32')\n",
    "feature_sel[-1,:]=np.array([k*1000,63960]).astype('int32')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_iterations=5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypothesis_feature = []\n",
    "hypothesis_Tau = []\n",
    "hypothesis_Toggle = []\n",
    "hypothesis_weights = []\n",
    "training_false_neg=[]\n",
    "training_false_pos=[]\n",
    "\n",
    "\n",
    "weights = np.ones(m) / m\n",
    "err_rtol=1e-02\n",
    "err_atol=1e-08\n",
    "\n",
    "false_neg_penalty=2\n",
    "false_pos_penalty=1\n",
    "\n",
    "for t in range(num_iterations):\n",
    "    k=0\n",
    "    print('Adaboost iteration:',t,', feature (x1000):',k)\n",
    "    result_final=\\\n",
    "    np.apply_along_axis(lambda x: decision_stump(x,y,m,weights,false_neg_penalty,false_pos_penalty,\\\n",
    "                                                     err_atol=err_atol,err_rtol=err_rtol),axis=0,\\\n",
    "                        arr=X[:,array_union[range(array_union_sel[k,0],array_union_sel[k,1])]])\n",
    "    k_upper=10\n",
    "    for k in range(1,k_upper) :\n",
    "\n",
    "        print('Adaboost iteration:',t,', feature (x1000):',k)\n",
    "        result_1e4=\\\n",
    "        np.apply_along_axis(lambda x: decision_stump(x,y,m,weights,false_neg_penalty,false_pos_penalty,\\\n",
    "                                                     err_atol=err_atol,err_rtol=err_rtol),axis=0,\\\n",
    "                        arr=X[:,array_union[range(array_union_sel[k,0],array_union_sel[k,1])]])\n",
    "        result_final = np.append(result_final,result_1e4,axis=1)\n",
    "        del result_1e4\n",
    "        \n",
    "    err_rtol=err_rtol/1000\n",
    "    err_atol=err_atol/1000\n",
    "    \n",
    "    min_err_feature = np.argmin(result_final[5,:])\n",
    "    \n",
    "    min_err_feature = np.where(np.isclose(result_final[5,:],\\\n",
    "                                                   np.min(result_final[5,:]),rtol=err_rtol, atol=err_atol))[0]\n",
    "    \n",
    "    #if len(min_err_feature) > 1:\n",
    "    min_err_feature = min_err_feature[np.argmax(result_final[2,min_err_feature])]\n",
    "    min_err_Tau=result_final[3,min_err_feature].astype('float')\n",
    "    min_err_Toggle=result_final[6,min_err_feature].astype('bool')\n",
    "    min_weighted_err_hypothesis=result_final[5,min_err_feature].astype('float')\n",
    "    min_err_false_neg=result_final[0,min_err_feature].astype('float')\n",
    "    min_err_false_pos=result_final[1,min_err_feature].astype('float')\n",
    "\n",
    "    ### \"!!! Use next line only for array_union !!!\"\n",
    "    ### \"!!! **** !!!\"\n",
    "    min_err_feature = array_union[min_err_feature]\n",
    "    \n",
    "    error_samples=(np.bitwise_xor(np.greater_equal(\\\n",
    "                        np.reshape(X[:,min_err_feature],-1),min_err_Tau),min_err_Toggle)!=(y>0))  \n",
    "    if t==0:\n",
    "        error_samples2 = error_samples\n",
    "    else:\n",
    "        error_samples2 = np.bitwise_xor(np.multiply(2*reduce(np.bitwise_xor,(np.greater_equal(\\\n",
    "                                X[:,np.array(hypothesis_feature)],np.array(hypothesis_Tau)[np.newaxis,:])\\\n",
    "                ,np.array(hypothesis_Toggle)[np.newaxis,:]))-1,np.array(hypothesis_weights)[np.newaxis,:]).\\\n",
    "sum(axis=1)>=0,y>0)\n",
    "        \n",
    "    min_weighted_err=reduce(np.multiply,(error_samples,weights,np.where(y==1,false_neg_penalty,false_pos_penalty\\\n",
    "                                             ))).sum(axis=0)\n",
    "    if min_weighted_err>0:\n",
    "        alpha = (np.log(1 - min_weighted_err) - np.log(min_weighted_err)) / 2\n",
    "    else:\n",
    "        alpha=0\n",
    "        t=num_iterations\n",
    "    \n",
    "    false_list=np.multiply(error_samples2,np.where(y==1,-10,10))\n",
    "    \n",
    "    print('accuracy:',1-np.sum(error_samples2>0)/1000,'FP:',np.sum(false_list==10),'FN:',np.sum(false_list==-10))\n",
    "    weights = weights * np.exp(- alpha * (2*error_samples-1))\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    hypothesis_feature.append(min_err_feature)\n",
    "    hypothesis_Tau.append(min_err_Tau)\n",
    "    hypothesis_Toggle.append(min_err_Toggle)                          \n",
    "    hypothesis_weights.append(alpha)\n",
    "    training_false_neg.append(min_err_false_neg)\n",
    "    training_false_pos.append(min_err_false_pos)\n",
    "y_pred=np.multiply(2*reduce(np.bitwise_xor,(np.greater_equal(\\\n",
    "                                X[:,np.array(hypothesis_feature)],np.array(hypothesis_Tau)[np.newaxis,:])\\\n",
    "                ,np.array(hypothesis_Toggle)[np.newaxis,:]))-1,np.array(hypothesis_weights)[np.newaxis,:]).\\\n",
    "sum(axis=1)>=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If toggle is 1 => hypothesis: y = 1 if feature_value<Threshold\n",
    "hypothesis_Toggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_false_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_false_pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
